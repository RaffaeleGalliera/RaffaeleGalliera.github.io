<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Raffaele Galliera</title> <meta name="author" content="Raffaele Galliera"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="multi-agent-reinforcement-learning, reinforcement-learning, communication-networks, distributed-systems"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon.png"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://raffaelegalliera.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Raffaele</span> Galliera </h1> <p class="desc">Ph.D. Student in the <a href="https://uwf.edu/intelligent-systems-and-robotics/" rel="external nofollow noopener" target="_blank">Intelligent Systems and Robotics</a> joint program between the University of West Florida and the <a href="https://www.ihmc.us/" rel="external nofollow noopener" target="_blank">Institute for Human and Machine Cognition (IHMC)</a>.</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/P1310997-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/P1310997-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/P1310997-1400.webp"></source> <img src="/assets/img/P1310997.jpg?5f898af97efa26c790fc2b444047def3" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="P1310997.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="address"> <p>NOMADS Lab</p> <p>40 S Alcaniz St</p> <p>Pensacola, Florida, USA, 32501</p> </div> </div> <div class="clearfix"> <p>I am a passionate researcher with a focus on multi-agent Reinforcement Learning for optimizing communication networks and learning communication protocols.</p> <p>While focusing on my Ph.D. I also intern as a Research Assistant at the NOMADS Lab (IHMC), where communication protocols and engineering solutions are designed for challenged networking environments. During my master’s degree, in 2021, I served as vice president of the Artificial Intelligence and Data Analytics group (AIDA) at UWF. We placed 3rd at the <a href="https://www.navy.mil/Resources/Blogs/Detail/Article/2570693/artificial-intelligence-tracks-at-sea-challenge-prairie-view-am-university-a-hi/" rel="external nofollow noopener" target="_blank">nationwide AI Tracks at Sea competition</a> organized by the NIWC Pacific.</p> <p>In 2023, I first-authored publications on single and multi-agent Reinforcement Learning and presented at the <a href="https://sites.google.com/view/rlready4prodworkshop/home" rel="external nofollow noopener" target="_blank">AAAI 2023 Reinforcement Learning Ready for Production Workshop</a> and at the <a href="mlforsystems.org"> NeurIPS 2023 Workshop on Machine Learning for Systems</a>. I have also been accepted at the <a href="https://aaai.org/aaai-conference/dc-24-program/" rel="external nofollow noopener" target="_blank">29th AAAI/SIGAI Doctoral Consortium</a> at AAAI 2024, where I presented my research on Deep Reinforcement Learning for Communication Networks.</p> <p>From April to October 2024 I participated in a 6-months internship for Apple, working on single and multi-agent Reinforcement Learning for the Wireless Technologies and Ecosystems team in San Diego.</p> <p>My academic journey began at the University of Ferrara, where I earned a Bachelor’s Degree in Electronics and Computer Science Engineering. A double degree program in Computer Science between the University of Ferrara and the University of West Florida led me to complete my Master of Science in Computer Science at UWF. After my Master’s Degree, I embarked on my Ph.D. in Intelligent Systems and Robotics, a joint program between IHMC and UWF, under the advice of <a href="https://www.ihmc.us/groups/nsuri/" rel="external nofollow noopener" target="_blank">Dr. Niranjan Suri</a> and <a href="https://www.ihmc.us/groups/bvenable/" rel="external nofollow noopener" target="_blank">Dr. Kristen Brent Venable</a>.</p> </div> <h2><a href="/news/" style="color: inherit;">news</a></h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Feb 2, 2024</th> <td> Starting on April 2024 I’ll take part to a 6-months internship at Apple working with the Wireless Technologies and Ecosystems team! </td> </tr> <tr> <th scope="row">Nov 16, 2023</th> <td> Our paper <a href="https://arxiv.org/abs/2308.16198" rel="external nofollow noopener" target="_blank">“Learning Collaborative Information Dissemination with Graph-based Multi-Agent Reinforcement Learning”</a> has been accepted at the <a href="mlforsystems.org"> NeurIPS 2023 Workshop on Machine Learning for Systems</a>! </td> </tr> <tr> <th scope="row">Oct 9, 2023</th> <td> I’ll be participating to the <a href="https://aaai.org/aaai-conference/dc-24-program/" rel="external nofollow noopener" target="_blank">29th AAAI/SIGAI Doctoral Consortium</a> at AAAI 2024! </td> </tr> <tr> <th scope="row">Aug 25, 2023</th> <td> Our pre-print “Learning Collaborative Information Dissemination with Graph-based Multi-Agent Reinforcement Learning” is available on <a href="https://arxiv.org/abs/2308.16198" rel="external nofollow noopener" target="_blank">arXiv</a>. </td> </tr> <tr> <th scope="row">Aug 7, 2023</th> <td> Our paper “Learning to Sail Dynamic Networks: The MARLIN Reinforcement Learning Framework for Congestion Control in Tactical Environments” has been accepted at MILCOM 2023! </td> </tr> <tr> <th scope="row">Dec 24, 2022</th> <td> Our paper “MARLIN: Soft Actor-Critic based Reinforcement Learning for Congestion Control in Real Networks” has been accepted at the 36th IEEE/IFIP Network Operations and Management Symposium (NOMS 2023)! </td> </tr> <tr> <th scope="row">Nov 19, 2022</th> <td> Our paper “MARLIN: Soft Actor-Critic based Reinforcement Learning for Congestion Control in Real Networks” has been accepted at the <a href="https://sites.google.com/view/rlready4prodworkshop/home" rel="external nofollow noopener" target="_blank">AAAI 2023 Reinforcement Learning Ready for Production Workshop</a>! </td> </tr> </table> </div> </div> <h2><a href="/publications/" style="color: inherit;">selected publications</a></h2> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="galliera2023learning" class="col-sm-8"> <div class="title">Collaborative Information Dissemination with Graph-Based Multi-Agent Reinforcement Learning</div> <div class="author"> <em>Raffaele Galliera</em>, Kristen Brent Venable, Matteo Bassani, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Niranjan Suri' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Algorithmic Decision Theory</em>, 2025 </div> <div class="periodical"> A previous version with experiments performed on static networks was presented at the NeurIPS 2023 Workshop on Machine Learning for Systems (poster attached below). </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2308.16198" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/RaffaeleGalliera/melissa" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/mlsys_2024_poster_portrait_final_a1.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-arxiv-id="2308.16198"></span> <span class="__dimensions_badge_embed__" data-doi="10.1007/978-3-031-73903-3_11" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Efficient information dissemination is crucial for supporting critical operations across domains like disaster response, autonomous vehicles, and sensor networks. This paper introduces a Multi-Agent Reinforcement Learning (MARL) approach as a significant step forward in achieving more decentralized, efficient, and collaborative information dissemination. We propose a Partially Observable Stochastic Game (POSG) formulation for information dissemination empowering each agent to decide on message forwarding independently, based on the observation of their one-hop neighborhood. This constitutes a significant paradigm shift from heuristics currently employed in real-world broadcast protocols. Our novel approach harnesses Graph Convolutional Reinforcement Learning and Graph Attention Networks (GATs) with dynamic attention to capture essential network features. We propose two approaches to accomplish cooperative information dissemination, L-DyAN and HL-DyAN, differing in terms of the information exchanged among agents. Our experimental results show that our trained policies outperform existing methods, including the state-of-the-art heuristic, in terms of network coverage and communication overhead on dynamic networks of varying density and behavior.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">galliera2023learning</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Galliera, Raffaele and Venable, Kristen Brent and Bassani, Matteo and Suri, Niranjan}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Freeman, Rupert and Mattei, Nicholas}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Collaborative Information Dissemination with Graph-Based Multi-Agent Reinforcement Learning}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Algorithmic Decision Theory}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer Nature Switzerland}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Cham}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{160--173}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{978-3-031-73903-3}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1007/978-3-031-73903-3_11}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2308.16198}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.LG}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="galliera2024aaaiDC" class="col-sm-8"> <div class="title">Deep Reinforcement Learning for Communication Networks</div> <div class="author"> <em>Raffaele Galliera</em> </div> <div class="periodical"> Feb 2024 </div> <div class="periodical"> Research abstract published at the 29th AAAI/SIGAI Doctoral Consortium. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/aaai_dc_2024_paper.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/pdf/aaai_dc_2024_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>This research explores optimizing communication tasks with (Multi-Agent) Reinforcement Learning (RL/MARL) in Point-to-Point and Group Communication (GC) networks. The study initially applied RL for Congestion Control in networks with dynamic link properties, yielding competitive results. Then, it focused on the challenge of effective message dissemination in GC networks, by framing a novel game-theoretic formulation and designing methods to solve the task based on MARL and Graph Convolution. Future research will deepen the exploration of MARL in GC. This will contribute to both academic knowledge and practical advancements in the next generation of communication protocols.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">galliera2024aaaiDC</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Deep Reinforcement Learning for Communication Networks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Galliera, Raffaele}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">feb</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="10154210" class="col-sm-8"> <div class="title">MARLIN: Soft Actor-Critic based Reinforcement Learning for Congestion Control in Real Networks</div> <div class="author"> <em>Raffaele Galliera</em>, Alessandro Morelli, Roberto Fronteddu, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Niranjan Suri' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In NOMS 2023-2023 IEEE/IFIP Network Operations and Management Symposium</em>, May 2023 </div> <div class="periodical"> Presented at the AAAI 2023 Workshop on Reinforcement Learning Ready for Production </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2302.01301" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/marlin_milcom_presentation.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1109/NOMS56928.2023.10154210"></span> <span class="__dimensions_badge_embed__" data-doi="10.1109/NOMS56928.2023.10154210" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Fast and efficient transport protocols are the foundation of an increasingly distributed world. The burden of continuously delivering improved communication performance to support next-generation applications and services, combined with the increasing heterogeneity of systems and network technologies, has promoted the design of Congestion Control (CC) algorithms that perform well under specific environments. The challenge of designing a generic CC algorithm that can adapt to a broad range of scenarios is still an open research question. To tackle this challenge, we propose to apply a novel Reinforcement Learning (RL) approach. Our solution, MARLIN, uses the Soft Actor-Critic algorithm to maximize both entropy and return and models the learning process as an infinite-horizon task. We trained MARLIN on a real network with varying background traffic patterns to overcome the sim-to-real mismatch that researchers have encountered when applying RL to CC. We evaluated our solution on the task of file transfer and compared it to TCP Cubic. While further research is required, results have shown that MARLIN can achieve comparable results to TCP with little hyperparameter tuning, in a task significantly different from its training setting. Therefore, we believe that our work represents a promising first step towards building CC algorithms based on the maximum entropy RL framework.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">10154210</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Galliera, Raffaele and Morelli, Alessandro and Fronteddu, Roberto and Suri, Niranjan}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{NOMS 2023-2023 IEEE/IFIP Network Operations and Management Symposium}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MARLIN: Soft Actor-Critic based Reinforcement Learning for Congestion Control in Real Networks}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1-10}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/NOMS56928.2023.10154210}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2374-9709}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Presented at the AAAI 2023 Workshop on Reinforcement Learning Ready for Production}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="galliera2023dynamic" class="col-sm-8"> <div class="title">Learning to Sail Dynamic Networks: The MARLIN Reinforcement Learning Framework for Congestion Control in Tactical Environments</div> <div class="author"> <em>Raffaele Galliera</em>, Mattia Zaccarini, Alessandro Morelli, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Roberto Fronteddu, Filippo Poltronieri, Niranjan Suri, Mauro Tortonesi' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>In MILCOM 2023 - 2023 IEEE Military Communications Conference (MILCOM)</em>, May 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2306.15591" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/RaffaeleGalliera/marlin-rlcc" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/marlin_milcom_presentation.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1109/MILCOM58377.2023.10356270"></span> <span class="__dimensions_badge_embed__" data-doi="10.1109/MILCOM58377.2023.10356270" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Conventional Congestion Control (CC) algorithms,such as TCP Cubic, struggle in tactical environments as they misinterpret packet loss and fluctuating network performance as congestion symptoms. Recent efforts, including our own MARLIN, have explored the use of Reinforcement Learning (RL) for CC, but they often fall short of generalization, particularly in competitive, unstable, and unforeseen scenarios. To address these challenges, this paper proposes an RL framework that leverages an accurate and parallelizable emulation environment to reenact the conditions of a tactical network. We also introduce refined RL formulation and performance evaluation methods tailored for agents operating in such intricate scenarios. We evaluate our RL learning framework by training a MARLIN agent in conditions replicating a bottleneck link transition between a Satellite Communication (SATCOM) and an UHF Wide Band (UHF) radio link. Finally, we compared its performance in file transfer tasks against Transmission Control Protocol (TCP) Cubic and the default strategy implemented in the Mockets tactical communication middleware. The results demonstrate that the MARLIN RL agent outperforms both TCP and Mockets under different perspectives and highlight the effectiveness of specialized RL solutions in optimizing CC for tactical network environments.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">galliera2023dynamic</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Galliera, Raffaele and Zaccarini, Mattia and Morelli, Alessandro and Fronteddu, Roberto and Poltronieri, Filippo and Suri, Niranjan and Tortonesi, Mauro}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{MILCOM 2023 - 2023 IEEE Military Communications Conference (MILCOM)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning to Sail Dynamic Networks: The MARLIN Reinforcement Learning Framework for Congestion Control in Tactical Environments}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{424-429}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/MILCOM58377.2023.10356270}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.LG}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="GALLIERA2022239" class="col-sm-8"> <div class="title">Object Detection at the Edge: Off-the-shelf Deep Learning Capable Devices and Accelerators</div> <div class="author"> <em>Raffaele Galliera</em>, and Niranjan Suri</div> <div class="periodical"> <em>Procedia Computer Science</em>, Sep 2022 </div> <div class="periodical"> 2022 International Conference on Military Communication and Information Systems (ICMCIS) </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="https://doi.org/10.1016/j.procs.2022.09.025"></span> <span class="__dimensions_badge_embed__" data-doi="https://doi.org/10.1016/j.procs.2022.09.025" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>The advent of energy efficient, embedded Deep Learning accelerators have brought inference capabilities in conjunction with Internet of Things devices in a pervasive manner. Once limited to passive data-gathering, such devices are now able to actively take part in data processing operations and predictive tasks with acceptable performance. Shifting such computation to the edge allows the creation of interconnected environments able to achieve efficient and low powered inference capabilities at the edge, without being dependent on external services in the cloud. Despite significant recent advancements, the field of Edge-ML is still maturing. Therefore, it is important to develop a framework to evaluate the performance of off-the-shelf hardware and state-of-the-art Deep Learning models suited for low-powered devices. Such a framework can be applied to new devices and models as they become available in the future. This paper describes such an evaluation framework, as well as a broad study of different Edge-ML devices, with comparisons in terms of performance, capabilities, limitations, and possible applications with a focus on deploying state-of-the-art Object Detection models. The end objective is enabling the deployment of low-latency and independent decision making processes in both civilian and military contexts.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">GALLIERA2022239</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Object Detection at the Edge: Off-the-shelf Deep Learning Capable Devices and Accelerators}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Procedia Computer Science}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{205}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{239-248}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{2022 International Conference on Military Communication and Information Systems (ICMCIS)}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1877-0509}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1016/j.procs.2022.09.025}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.sciencedirect.com/science/article/pii/S1877050922008900}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Galliera, Raffaele and Suri, Niranjan}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{IoT, Object Detection, EdgeML, Edge Computing, Deep Learning}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="delta21" class="col-sm-8"> <div class="title">Marine Vessel Tracking using a Monocular Camera</div> <div class="author"> Tobias Jacob., Raffaele Galliera., Muddasar Ali., and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Sikha Bagui.' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Proceedings of the 2nd International Conference on Deep Learning Theory and Applications - DeLTA</em>, Jun 2021 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2108.10367" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.5220/0010516000170028"></span> <span class="__dimensions_badge_embed__" data-doi="10.5220/0010516000170028" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">delta21</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jacob., Tobias and Galliera., Raffaele and Ali., Muddasar and Bagui., Sikha}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Marine Vessel Tracking using a Monocular Camera}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 2nd International Conference on Deep Learning Theory and Applications - DeLTA}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{17-28}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{SciTePress}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{INSTICC}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.5220/0010516000170028}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{978-989-758-526-5}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2184-9277}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%72%67%61%6C%6C%69%65%72%61@%69%68%6D%63.%6F%72%67" title="email"><i class="fas fa-envelope"></i></a> <a href="https://orcid.org/0000-0001-7777-3835" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=Rja5TFIAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.semanticscholar.org/author/2121362149" title="Semantic Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-semantic-scholar"></i></a> <a href="https://www.researchgate.net/profile/Raffaele-Galliera/" title="ResearchGate" rel="external nofollow noopener" target="_blank"><i class="ai ai-researchgate"></i></a> <a href="https://github.com/RaffaeleGalliera" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/raffaele-galliera-819780137" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a> <a href="/feed.xml" title="RSS Feed"><i class="fas fa-rss-square"></i></a> </div> <div class="contact-note"> </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Raffaele Galliera. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>